{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import fiona.transform\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from affine import Affine\n",
    "\n",
    "from utils.dask import upload_source\n",
    "from utils.dask import create_cluster\n",
    "\n",
    "from utils.hls.compute import process_catalog\n",
    "from utils import get_logger\n",
    "\n",
    "from utils.hls import catalog\n",
    "from utils.hls import compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger('hls-az')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_zr = fsspec.get_mapper(\n",
    "    f\"az://fia/catalogs/fia_az_2015-2019.zarr\",\n",
    "    account_name=\"usfs\",\n",
    "    account_key=os.environ['AZURE_ACCOUNT_KEY']\n",
    ")\n",
    "ds_az = xr.open_zarr(az_zr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_az = ds_az.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_az.drop_duplicates(subset=['INDEX'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_az_500 = df_az[500:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chip_tiles(job_id, job_df, job_groupby, chunks, write_store, client):\n",
    "    \"\"\"A job compatible with `process_catalog` which .\n",
    "    Args:\n",
    "        job_id (str): Id of the job, used for tracking purposes\n",
    "        job_df (pandas.Dataframe): Dataframe of  to include in the computation\n",
    "        job_groupby (str): How to group the dataset produced from the dataframe (e.g. \"time.month\")\n",
    "        bands (List[HLSBand]): List of HLSBand objects to compute median reflectance on\n",
    "        band_names (List[str]): List of band name strings\n",
    "        qa_band_name (str): Name of the QA band to use for masking\n",
    "        chunks (Dict[str, int]): How to chunk HLS input data\n",
    "        write_store (fsspec.FSMap): The location to write any results\n",
    "        client (dask.distributed.Client): Dask cluster client to submit tasks to\n",
    "    Returns:\n",
    "        dask.distributed.Future: Future for the computation that is being done, can be waited on.\n",
    "    \"\"\"\n",
    "    def chip(ds, lat, lon, chip_size):\n",
    "        CRS = \"EPSG:4326\"\n",
    "        tfm = Affine(*ds.attrs['transform'])\n",
    "        ([x], [y]) = fiona.transform.transform(\n",
    "            CRS, ds.attrs['crs'], [lon], [lat]\n",
    "        )\n",
    "        print(x, y)\n",
    "        x_idx, y_idx = [round(coord) for coord in ~tfm * (x, y)]\n",
    "        print(x_idx, y_idx)\n",
    "        half_chip = int(chip_size/2)\n",
    "        return ds[dict(x=range(x_idx-half_chip, x_idx+half_chip), y=range(y_idx-half_chip, y_idx+half_chip))].drop_vars(['COASTAL_AEROSOL', 'CIRRUS'])\n",
    "\n",
    "    chip_size = 32\n",
    "    year_points = list(job_df.groupby(\"year\"))\n",
    "    futures = []\n",
    "    for year, points in year_points:\n",
    "        # read the zarr for the job_id (tile) + year\n",
    "        zr = fsspec.get_mapper(\n",
    "            f\"az://fia/hls/{float(year)}/{job_id}.zarr\",\n",
    "            account_name=\"usfs\",\n",
    "            account_key=os.environ['AZURE_ACCOUNT_KEY']\n",
    "        )\n",
    "        print(year)\n",
    "        ds = client.submit(xr.open_zarr, zr, chunks=chunks)\n",
    "        # calculate chip for each point in points\n",
    "        for _, point_row in points.iterrows():\n",
    "            lat = point_row['lat']\n",
    "            lon = point_row['lon']\n",
    "            sample = client.submit(chip, ds, lat, lon, chip_size)\n",
    "            write_store = fsspec.get_mapper(\n",
    "                f\"az://fia/chips-test6/hls/az/HLSAZ{int(year)}{int(point_row['INDEX'])}.zarr\",\n",
    "                account_name=\"usfs\",\n",
    "                account_key=os.environ['AZURE_ACCOUNT_KEY']\n",
    "            )\n",
    "            futures.append(client.submit(compute.save_to_zarr, sample, write_store, 'w', point_row['INDEX']))\n",
    "    return client.submit(lambda x: job_id, futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 32\n",
    "cluster = create_cluster(\n",
    "    workers=num_workers,\n",
    "    worker_threads=1,\n",
    "    worker_memory=4,\n",
    "    scheduler_threads=1,\n",
    "    scheduler_memory=8\n",
    ")\n",
    "client = cluster.get_client()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Waiting for cluster workers to start\")\n",
    "client.wait_for_workers(num_workers)\n",
    "logger.info(\"Uploading code to workers\")\n",
    "upload_source('./utils', client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_groupby = \"tile\"\n",
    "job_groupby = \"year\"\n",
    "chunks = {'month': 1, 'x': 3660, 'y': 3660}\n",
    "process_catalog(\n",
    "    catalog=df_az_500,\n",
    "    catalog_groupby=catalog_groupby,\n",
    "    job_fn=chip_tiles,\n",
    "    job_groupby=job_groupby,\n",
    "    chunks=chunks,\n",
    "    account_name=\"usfs\",\n",
    "    storage_container=\"fia/chips-test6/hls/az\",\n",
    "    account_key=os.environ['AZURE_ACCOUNT_KEY'],\n",
    "    client=client,\n",
    "    concurrency=4,\n",
    "    checkpoint_path=\"./checkpoint_file.txt\",\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
