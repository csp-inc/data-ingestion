{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# pip/conda installed\n",
    "import dask.array as da\n",
    "import fsspec\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from dask.distributed import as_completed\n",
    "from dask.distributed import Client\n",
    "from dask_gateway import GatewayCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.hls import HLSBand\n",
    "from utils.hls import HLSCatalog\n",
    "from utils.hls import scene_to_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup necessary utility functions/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(qa_band):\n",
    "    \"\"\"Takes a data array HLS qa band and returns a mask of True where quality is good, False elsewhere\n",
    "    Mask usage:\n",
    "        ds.where(mask)\n",
    "        \n",
    "    Example:\n",
    "        qa_mask = get_mask(dataset[HLSBand.QA])\n",
    "        ds = dataset.drop_vars(HLSBand.QA)\n",
    "        masked = ds.where(qa_mask)\n",
    "    \"\"\"\n",
    "    def is_bad_quality(qa):\n",
    "        cirrus = 0b1\n",
    "        cloud = 0b10\n",
    "        adjacent_cloud = 0b100\n",
    "        cloud_shadow = 0b1000\n",
    "        high_aerosol = 0b11000000\n",
    "\n",
    "        return (qa & cirrus > 0) | (qa & cloud > 0) | (qa & adjacent_cloud > 0) | \\\n",
    "            (qa & cloud_shadow > 0) | (qa & high_aerosol == high_aerosol)\n",
    "    return xr.where(is_bad_quality(qa_band), False, True)  # True where is_bad_quality is False, False where is_bad_quality is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_band_url(tpl):\n",
    "    \"\"\"Fetch a given url with xarray, creating a dataset with a single data variable of the band name for the url.\n",
    "    \n",
    "    Args:\n",
    "        tpl (Tuple[str, str]): tuple of the form (band, url) - the url to fetch and the band name for the data variable\n",
    "        \n",
    "    Returns:\n",
    "        xarray.Dataset: Dataset for the given HLS scene url with the data variable being named the given band\n",
    "        \n",
    "    \"\"\"\n",
    "    band, url = tpl\n",
    "    da = xr.open_rasterio(url, chunks=chunks)\n",
    "    da = da.squeeze().drop(labels='band')\n",
    "    return da.to_dataset(name=band)\n",
    "\n",
    "def compute_tile_median(job_id, ds, groupby, qa_name, write_store):\n",
    "    \"\"\"Compute QA-band-masked {groupby} median reflectance for the given dataset and save the result as zarr to `write_store`.\n",
    "    \n",
    "    Args:\n",
    "        job_id (str): The job_id of the tile being computed\n",
    "        ds (xarray.Dataset): Dataset to compute on\n",
    "        groupby (str): How to group the dataset (e.g. \"time.month\")\n",
    "        qa_name (str): Name of the QA band to use for masking\n",
    "        write_store (fsspec.FSMap): The location to write the zarr\n",
    "    \n",
    "    Returns:\n",
    "        str: The job_id that was computed and written\n",
    "        \n",
    "    \"\"\"\n",
    "    # apply QA mask\n",
    "    if qa_name in ds.data_vars:\n",
    "        qa_mask = get_mask(ds[qa_name])\n",
    "        ds = (ds\n",
    "            .drop_vars(qa_name)  # drop QA band\n",
    "            .where(qa_mask)  # Apply mask\n",
    "        )\n",
    "    zarr = (ds\n",
    "        .where(ds != -1000)  # -1000 means no data - set those entries to nan\n",
    "        .groupby(groupby)\n",
    "        .median()\n",
    "        .chunk({'month': 1, 'y': 3660, 'x': 3660})  # groupby + median changes chunk size...lets change it back\n",
    "        .to_zarr(write_store, mode='w')\n",
    "    )\n",
    "    return job_id\n",
    "\n",
    "def calculate_job_median(job_id, job_df, job_groupby, bands, band_names, qa_band_name, write_store, client):\n",
    "    \"\"\"A job compatible with `process_catalog` which computes per-band median reflectance for the input job_df.\n",
    "    \n",
    "    Args:\n",
    "        job_id (str): Id of the job, used for tracking purposes\n",
    "        job_df (pandas.Dataframe): Dataframe of scenes to include in the computation\n",
    "        job_groupby (str): How to group the dataset produced from the dataframe (e.g. \"time.month\")\n",
    "        bands (List[HLSBand]): List of HLSBand objects to compute median reflectance on\n",
    "        band_names (List[str]): List of band name strings\n",
    "        qa_band_name (str): Name of the QA band to use for masking\n",
    "        write_store (fsspec.FSMap): The location to write any results\n",
    "        client (dask.distributed.Client): Dask cluster client to submit tasks to\n",
    "        \n",
    "    Returns:\n",
    "        dask.distributed.Future: Future for the computation that is being done, can be waited on.\n",
    "        \n",
    "    \"\"\"\n",
    "    scene_ds_futures = []\n",
    "    for _, row in job_df.iterrows():\n",
    "        scenes = scene_to_urls(row['scene'], row['sensor'], bands)\n",
    "        # list of datasets that need to be xr.merge'd (future)\n",
    "        band_ds_futures = client.map(fetch_band_url, list(zip(band_names, scenes)), priority=0)\n",
    "        # single dataset with every band (future)\n",
    "        scene_ds_futures.append(client.submit(xr.merge, band_ds_futures, priority=1))\n",
    "    # dataset of a single index/tile with a data var for every band and dimensions: x, y, time\n",
    "    job_ds_future = client.submit(lambda scene_futures: xr.concat(scene_futures, dim=pd.DatetimeIndex(job_df['dt'].tolist(), name='time')), scene_ds_futures, priority=2)\n",
    "    # compute masked, monthly, median per band per pixel\n",
    "    return client.submit(compute_tile_median, job_id, job_ds_future, job_groupby, qa_band_name, write_store, priority=3)\n",
    "\n",
    "def process_catalog(\n",
    "    catalog,\n",
    "    catalog_groupby,\n",
    "    job_fn,\n",
    "    job_groupby,\n",
    "    account_name,\n",
    "    storage_container,\n",
    "    account_key,\n",
    "    client,\n",
    "):\n",
    "    \"\"\"Process a catalog.\n",
    "    \n",
    "    Args:\n",
    "        catalog (HLSCatalog): catalog to process\n",
    "        catalog_groupby (str): column to group the catalog in to jobs by (e.g. 'INDEX', 'tile')\n",
    "        job_fn: a function to apply to each job from the grouped catalog (e.g. `calculate_job_median`)\n",
    "        job_groupby (str): how to group data built within each job (e.g. 'time.month', 'time.year')\n",
    "        account_name (str): Azure storage account to write results to\n",
    "        storage_container (str): Azure storage container within the `account_name` to write results to\n",
    "        account_key (str): Azure account key for the `account_name` which results are written to\n",
    "        client (dask.distributed.Client): Dask cluster client to submit tasks to\n",
    "        \n",
    "    \"\"\"\n",
    "    bands = point_catalog.xr_ds.attrs['bands']\n",
    "    band_names = [band.name for band in bands]\n",
    "    qa_band_name = HLSBand.QA.name\n",
    "\n",
    "    df = catalog.xr_ds.to_dataframe()\n",
    "    job_futures = []\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for job_id, job_df in df.reset_index().groupby(catalog_groupby):\n",
    "        write_store = fsspec.get_mapper(\n",
    "            f\"az://{storage_container}/{job_id}.zarr\",\n",
    "            account_name=account_name,\n",
    "            account_key=account_key\n",
    "        )\n",
    "        job_futures.append(\n",
    "            job_fn(job_id, job_df, job_groupby, bands, band_names, qa_band_name, write_store, client)\n",
    "        )\n",
    "    for future in as_completed(job_futures):\n",
    "        print(future.result())\n",
    "    print(f\"{len(job_futures)} completed in {time.perf_counter()-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HLS data on Azure isn't tiled so we want to read the entire data once (each tile is 3660x3660)...\n",
    "x_chunk = 3660\n",
    "y_chunk = 3660\n",
    "chunks = {'band': 1, 'x': x_chunk, 'y': y_chunk}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with your account key\n",
    "os.environ['AZURE_ACCOUNT_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_url = fsspec.get_mapper(\n",
    "    f\"az://fia/catalogs/fia10.zarr\",\n",
    "    account_name=\"usfs\",\n",
    "    account_key=os.environ['AZURE_ACCOUNT_KEY']\n",
    ")\n",
    "point_catalog = HLSCatalog.from_zarr(catalog_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster dashboard visible at: /services/dask-gateway/clusters/default.8f095b12e15e4aeb969e05609e1506db/status\n",
      "8\n",
      "5\n",
      "2\n",
      "3 completed in 349.21651702599775 seconds\n"
     ]
    }
   ],
   "source": [
    "account_name=\"usfs\"\n",
    "storage_container=\"fia/hls-testing\"\n",
    "account_key=os.environ[\"AZURE_ACCOUNT_KEY\"]\n",
    "catalog_groupby = \"INDEX\"\n",
    "job_groupby = \"time.month\"\n",
    "\n",
    "with GatewayCluster(worker_cores=2, worker_memory=8) as cluster:\n",
    "    print(f\"Cluster dashboard visible at: {cluster.dashboard_link}\")\n",
    "    cluster.scale(16)\n",
    "    client = cluster.get_client()\n",
    "    process_catalog(point_catalog, catalog_groupby, calculate_job_median, job_groupby, account_name, storage_container, account_key, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "1. Do QA on results\n",
    "1. Is COG data tiled now?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
