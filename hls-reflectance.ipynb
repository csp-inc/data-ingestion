{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# pip/conda installed\n",
    "import dask.array as da\n",
    "import fsspec\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from dask.distributed import Client\n",
    "from dask_gateway import GatewayCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.hls import HLSBand\n",
    "from utils.hls import HLSCatalog\n",
    "from utils.hls import scene_to_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup necessary utility functions/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiband_dataset(row, bands, chunks):\n",
    "    '''A function to load multiple bands into an xarray dataset adapted from https://github.com/scottyhq/cog-benchmarking/blob/master/notebooks/landsat8-cog-ndvi-mod.ipynb'''\n",
    "    datasets = []\n",
    "    for band, url in zip(bands, scene_to_urls(row['scene'], row['sensor'], bands)):\n",
    "        da = xr.open_rasterio(url, chunks=chunks)\n",
    "        da = da.squeeze().drop(labels='band')\n",
    "        datasets.append(da.to_dataset(name=band))\n",
    "    return xr.merge(datasets)\n",
    "\n",
    "def create_timeseries_multiband_dataset(df, bands, chunks):\n",
    "    '''For a single HLS tile create a multi-date, multi-band xarray dataset'''\n",
    "    datasets = []\n",
    "    for i,row in df.iterrows():\n",
    "        try:\n",
    "            ds = create_multiband_dataset(row, bands, chunks)\n",
    "            datasets.append(ds)\n",
    "        except Exception as e:\n",
    "            print('ERROR loading, skipping acquistion!')\n",
    "            print(e)\n",
    "    DS = xr.concat(datasets, dim=pd.DatetimeIndex(df['dt'].tolist(), name='time'))\n",
    "    print('Dataset size (Gb): ', DS.nbytes/1e9)\n",
    "    return DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(qa_band):\n",
    "    \"\"\"Takes a data array HLS qa band and returns a mask of True where quality is good, False elsewhere\n",
    "    Mask usage:\n",
    "        ds.where(mask)\n",
    "        \n",
    "    Example:\n",
    "        qa_mask = get_mask(dataset[HLSBand.QA])\n",
    "        ds = dataset.drop_vars(HLSBand.QA)\n",
    "        masked = ds.where(qa_mask)\n",
    "    \"\"\"\n",
    "    def is_bad_quality(qa):\n",
    "        cirrus = 0b1\n",
    "        cloud = 0b10\n",
    "        adjacent_cloud = 0b100\n",
    "        cloud_shadow = 0b1000\n",
    "        high_aerosol = 0b11000000\n",
    "\n",
    "        return (qa & cirrus > 0) | (qa & cloud > 0) | (qa & adjacent_cloud > 0) | \\\n",
    "            (qa & cloud_shadow > 0) | (qa & high_aerosol == high_aerosol)\n",
    "    return xr.where(is_bad_quality(qa_band), False, True)  # True where is_bad_quality is False, False where is_bad_quality is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_catalog(catalog, catalog_groupby, job_fn, job_groupby, account_name, storage_container, account_key):\n",
    "    \"\"\"Process a catalog.\n",
    "    \n",
    "    Args:\n",
    "        catalog (HLSCatalog): catalog to process\n",
    "        catalog_groupby (str): column to group the catalog in to jobs by (e.g. 'INDEX', 'year')\n",
    "        job_fn: a function to apply to each job from the grouped catalog (e.g. `calculate_tile_median`)\n",
    "        job_groupby (str): how to group data built within each job (e.g. 'time.month', 'time.year')\n",
    "    \"\"\"\n",
    "    grps = list(catalog.xr_ds.groupby(catalog_groupby))\n",
    "    \n",
    "    for idx, ds in grps:\n",
    "        df = ds.to_dataframe()\n",
    "        job_id = idx\n",
    "        write_store = fsspec.get_mapper(\n",
    "            f\"az://{storage_container}/{job_id}.zarr\",\n",
    "            account_name=account_name,\n",
    "            account_key=account_key\n",
    "        )\n",
    "        print(f\"Starting {job_id}\")\n",
    "        start = time.perf_counter()\n",
    "        # compute job and write to Azure blob storage\n",
    "        job_fn(job_id, df, catalog.xr_ds.attrs['bands'], job_groupby).to_zarr(write_store)\n",
    "        print(f\"{job_id} finished in {time.perf_counter()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tile_median(job_id, dataframe, bands, groupby):\n",
    "    tile_ds = create_timeseries_multiband_dataset(dataframe, bands, chunks)\n",
    "    # apply QA mask\n",
    "    if HLSBand.QA in tile_ds.data_vars:\n",
    "        qa_mask = get_mask(tile_ds[HLSBand.QA])\n",
    "        tile_ds = (tile_ds\n",
    "            .drop_vars(HLSBand.QA)  # drop QA band\n",
    "            .where(qa_mask)  # Apply mask\n",
    "        )\n",
    "    return (tile_ds\n",
    "        .where(tile_ds != -1000)  # -1000 means no data - set those entries to nan\n",
    "        .groupby(groupby)\n",
    "        .median()\n",
    "        .chunk({'month': 1, 'y': 3660, 'x': 3660})  # groupby + median changes chunk size...lets change it back\n",
    "        .rename({var: var.name for var in tile_ds.data_vars})  # Rename vars from Enum to string for saving to zarr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HLS data on Azure isn't tiled so we want to read the entire data once (each tile is 3660x3660)...\n",
    "x_chunk = 3660\n",
    "y_chunk = 3660\n",
    "chunks = {'band': 1, 'x': x_chunk, 'y': y_chunk}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AZURE_ACCOUNT_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with GatewayCluster(worker_cores=2, worker_memory=8) as cluster:\n",
    "    client = cluster.get_client()\n",
    "    catalog_url = fsspec.get_mapper(\n",
    "        f\"az://fia/catalogs/fia10.zarr\",\n",
    "        account_name=\"usfs\",\n",
    "        account_key=os.environ['AZURE_ACCOUNT_KEY']\n",
    "    )\n",
    "    point_catalog = HLSCatalog.from_zarr(catalog_url)\n",
    "    process_catalog(\n",
    "        catalog=point_catalog,\n",
    "        catalog_groupby='INDEX',\n",
    "        job_fn=calculate_tile_median,\n",
    "        job_groupby='time.month',\n",
    "        account_name=\"usfs\",\n",
    "        storage_container=\"fia/hls\",\n",
    "        account_key=os.environ[\"AZURE_ACCOUNT_KEY\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
